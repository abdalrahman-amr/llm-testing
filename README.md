
# Evaluating Large Language Models for Automated Unit Test Generation

This project investigates the effectiveness of recent Large Language Models (LLMs) in generating Python unit tests, and compares their performance to a traditional automated testing tool, Pynguin.

## ğŸ“Œ Overview

- **Goal**: Evaluate the correctness, coverage, and failure behavior of unit tests generated by five state-of-the-art LLMs compared to Pynguin.
- **Dataset**: [CodeRM-UnitTest](https://huggingface.co/datasets/KAKA22/CodeRM-UnitTest)
- **LLMs Used**:
  1. meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
  2. Qwen/Qwen2.5-Coder-32B-Instruct
  3. meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
  4. meta-llama/Llama-3.2-3B-Instruct-Turbo
  5. meta-llama/Llama-3.3-70B-Instruct-Turbo

All models were accessed via the TogetherAI API.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_code/               # Original functions from CodeRM dataset
â”‚   â”œâ”€â”€ generated_tests/        # LLM-generated test files
â”‚   â””â”€â”€ pynguin_tests/          # Tests generated by Pynguin
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ pytest_outputs/         # Pytest results (pass/fail logs)
â”‚   â””â”€â”€ analysis.ipynb          # Scripts for statistical analysis
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ test_generation_prompts.txt
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ generate_tests.py       # LLM interfacing script
â”‚   â”œâ”€â”€ run_pynguin.py          # Pynguin wrapper
â”‚   â””â”€â”€ evaluate_tests.py       # Test execution and result parsing
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âš™ï¸ Setup

1. Clone the repository:
   ```bash
   git clone <your-private-repo-url>
   cd <project-folder>
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Setup Pynguin (optional):
   ```bash
   pip install pynguin
   ```

4. Add your TogetherAI API key in `.env` or directly in `generate_tests.py`.

## ğŸš€ How to Run

### Generate Tests using LLMs:
```bash
python scripts/generate_tests.py
```

### Generate Tests using Pynguin:
```bash
python scripts/run_pynguin.py
```

### Run Evaluation:
```bash
python scripts/evaluate_tests.py
```

## ğŸ“Š Evaluation Criteria

- **Pass Rate**: Number of tests passing without runtime errors.
- **Code Coverage**: Measured using `coverage.py`.
- **Precision**: Proportion of valid assertions.
- **Failure Reason Classification**: Syntax errors, runtime exceptions, invalid test logic, etc.



## ğŸ‘¨â€ğŸ’» Authors

- Abdelrahman Amr Effat  
- Ibrahim Nasser Darwish  
- Abdrelrahman Mostafa Rifaat

