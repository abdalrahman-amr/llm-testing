
# Evaluating Large Language Models for Automated Unit Test Generation

This project investigates the effectiveness of recent Large Language Models (LLMs) in generating Python unit tests, and compares their performance to a traditional automated testing tool, Pynguin.

## 📌 Overview

- **Goal**: Evaluate the correctness, coverage, and failure behavior of unit tests generated by five state-of-the-art LLMs compared to Pynguin.
- **Dataset**: [CodeRM-UnitTest](https://huggingface.co/datasets/KAKA22/CodeRM-UnitTest)
- **LLMs Used**:
  1. meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8
  2. Qwen/Qwen2.5-Coder-32B-Instruct
  3. meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo
  4. meta-llama/Llama-3.2-3B-Instruct-Turbo
  5. meta-llama/Llama-3.3-70B-Instruct-Turbo

All models were accessed via the TogetherAI API.

## 📁 Project Structure

```
.
├── data/
│   ├── raw_code/               # Original functions from CodeRM dataset
│   ├── generated_tests/        # LLM-generated test files
│   └── pynguin_tests/          # Tests generated by Pynguin
├── evaluation/
│   ├── pytest_outputs/         # Pytest results (pass/fail logs)
│   └── analysis.ipynb          # Scripts for statistical analysis
├── prompts/
│   └── test_generation_prompts.txt
├── scripts/
│   ├── generate_tests.py       # LLM interfacing script
│   ├── run_pynguin.py          # Pynguin wrapper
│   └── evaluate_tests.py       # Test execution and result parsing
├── requirements.txt
└── README.md
```

## ⚙️ Setup

1. Clone the repository:
   ```bash
   git clone <your-private-repo-url>
   cd <project-folder>
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Setup Pynguin (optional):
   ```bash
   pip install pynguin
   ```

4. Add your TogetherAI API key in `.env` or directly in `generate_tests.py`.

## 🚀 How to Run

### Generate Tests using LLMs:
```bash
python scripts/generate_tests.py
```

### Generate Tests using Pynguin:
```bash
python scripts/run_pynguin.py
```

### Run Evaluation:
```bash
python scripts/evaluate_tests.py
```

## 📊 Evaluation Criteria

- **Pass Rate**: Number of tests passing without runtime errors.
- **Code Coverage**: Measured using `coverage.py`.
- **Precision**: Proportion of valid assertions.
- **Failure Reason Classification**: Syntax errors, runtime exceptions, invalid test logic, etc.



## 👨‍💻 Authors

- Abdelrahman Amr Effat  
- Ibrahim Nasser Darwish  
- Abdrelrahman Mostafa Rifaat

